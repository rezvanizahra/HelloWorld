{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "04-feedforward-nn.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rezvanizahra/HelloWorld/blob/master/04_feedforward_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGxhfqm1Bw7P"
      },
      "source": [
        "# Jovian Commit Essentials\n",
        "# Please retain and execute this cell without modifying the contents for `jovian.commit` to work\n",
        "!pip install jovian --upgrade -q\n",
        "import jovian\n",
        "jovian.set_project('04-feedforward-nn')\n",
        "jovian.set_colab_id('1Ru8LMFR7xmcJQkX7MZkk59dThoKPfKeS')"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0sQkmyWpeT4"
      },
      "source": [
        "# Training Deep Neural Networks on a GPU with PyTorch\n",
        "\n",
        "### Part 4 of \"Deep Learning with Pytorch: Zero to GANs\"\n",
        "\n",
        "This tutorial series is a hands-on beginner-friendly introduction to deep learning using [PyTorch](https://pytorch.org), an open-source neural networks library. These tutorials take a practical and coding-focused approach. The best way to learn the material is to execute the code and experiment with it yourself. Check out the full series here:\n",
        "\n",
        "1. [PyTorch Basics: Tensors & Gradients](https://jovian.ai/aakashns/01-pytorch-basics)\n",
        "2. [Gradient Descent & Linear Regression](https://jovian.ai/aakashns/02-linear-regression)\n",
        "3. [Working with Images & Logistic Regression](https://jovian.ai/aakashns/03-logistic-regression) \n",
        "4. [Training Deep Neural Networks on a GPU](https://jovian.ai/aakashns/04-feedforward-nn)\n",
        "5. [Image Classification using Convolutional Neural Networks](https://jovian.ai/aakashns/05-cifar10-cnn)\n",
        "6. [Data Augmentation, Regularization and ResNets](https://jovian.ai/aakashns/05b-cifar10-resnet)\n",
        "7. [Generating Images using Generative Adversarial Networks](https://jovian.ai/aakashns/06b-anime-dcgan/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRMQYXirpeT5"
      },
      "source": [
        " This tutorial covers the following topics:\n",
        " \n",
        " * Creating a deep neural network with hidden layers\n",
        " * Using a non-linear activation function\n",
        " * Using a GPU (when available) to speed up training\n",
        " * Experimenting with hyperparameters to improve the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JI69VLFpeT5"
      },
      "source": [
        "### How to run the code\n",
        "\n",
        "This tutorial is an executable [Jupyter notebook](https://jupyter.org) hosted on [Jovian](https://www.jovian.ai). You can _run_ this tutorial and experiment with the code examples in a couple of ways: *using free online resources* (recommended) or *on your computer*.\n",
        "\n",
        "#### Option 1: Running using free online resources (1-click, recommended)\n",
        "\n",
        "The easiest way to start executing the code is to click the **Run** button at the top of this page and select **Run on Colab**. [Google Colab](https://colab.research.google.com) is a free online platform for running Jupyter notebooks using Google's cloud infrastructure. You can also select \"Run on Binder\" or \"Run on Kaggle\" if you face issues running the notebook on Google Colab. \n",
        "\n",
        "\n",
        "#### Option 2: Running on your computer locally\n",
        "\n",
        "To run the code on your computer locally, you'll need to set up [Python](https://www.python.org), download the notebook and install the required libraries. We recommend using the [Conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/) distribution of Python. Click the **Run** button at the top of this page, select the **Run Locally** option, and follow the instructions.\n",
        "\n",
        ">  **Jupyter Notebooks**: This tutorial is a [Jupyter notebook](https://jupyter.org) - a document made of _cells_. Each cell can contain code written in Python or explanations in plain English. You can execute code cells and view the results, e.g., numbers, messages, graphs, tables, files, etc., instantly within the notebook. Jupyter is a powerful platform for experimentation and analysis. Don't be afraid to mess around with the code & break things - you'll learn a lot by encountering and fixing errors. You can use the \"Kernel > Restart & Clear Output\" or \"Edit > Clear Outputs\" menu option to clear all outputs and start again from the top."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdphVwCopeT5"
      },
      "source": [
        "### Using a GPU for faster training\n",
        "\n",
        "You can use a [Graphics Processing Unit](https://en.wikipedia.org/wiki/Graphics_processing_unit) (GPU) to train your models faster if your execution platform is connected to a GPU manufactured by NVIDIA. Follow these instructions to use a GPU on the platform of your choice:\n",
        "\n",
        "* _Google Colab_: Use the menu option \"Runtime > Change Runtime Type\" and select \"GPU\" from the \"Hardware Accelerator\" dropdown.\n",
        "* _Kaggle_: In the \"Settings\" section of the sidebar, select \"GPU\" from the \"Accelerator\" dropdown. Use the button on the top-right to open the sidebar.\n",
        "* _Binder_: Notebooks running on Binder cannot use a GPU, as the machines powering Binder aren't connected to any GPUs.\n",
        "* _Linux_: If your laptop/desktop has an NVIDIA GPU (graphics card), make sure you have installed the [NVIDIA CUDA drivers](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html).\n",
        "* _Windows_: If your laptop/desktop has an NVIDIA GPU (graphics card), make sure you have installed the [NVIDIA CUDA drivers](https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html).\n",
        "* _macOS_: macOS is not compatible with NVIDIA GPUs\n",
        "\n",
        "\n",
        "If you do not have access to a GPU or aren't sure what it is, don't worry, you can execute all the code in this tutorial just fine without a GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1N-aPtrpeT5"
      },
      "source": [
        "## Preparing the Data\n",
        "\n",
        "In [the previous tutorial](https://jovian.ai/aakashns/03-logistic-regression), we trained a logistic regression model to identify handwritten digits from the MNIST dataset with an accuracy of around 86%. The dataset consists of 28px by 28px grayscale images of handwritten digits (0 to 9) and labels for each image indicating which digit it represents. Here are some sample images from the dataset:\n",
        "\n",
        "![mnist-sample](https://i.imgur.com/CAYnuo1.jpg)\n",
        "\n",
        "We noticed that it's quite challenging to improve the accuracy of a logistic regression model beyond 87%, since the model assumes a linear relationship between pixel intensities and image labels. In this post, we'll try to improve upon it  using a *feed-forward neural network* which can capture non-linear relationships between inputs and targets.\n",
        "\n",
        "Let's begin by installing and importing the required modules and classes from `torch`, `torchvision`, `numpy`, and `matplotlib`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28gVv8fbpeT5"
      },
      "source": [
        "# Uncomment and run the appropriate command for your operating system, if required\n",
        "\n",
        "# Linux / Binder\n",
        "# !pip install numpy matplotlib torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# Windows\n",
        "# !pip install numpy matplotlib torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# MacOS\n",
        "# !pip install numpy matplotlib torch torchvision torchaudio"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WapU423CpeT5"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "%matplotlib inline\n",
        "\n",
        "# Use a white background for matplotlib figures\n",
        "matplotlib.rcParams['figure.facecolor'] = '#ffffff'"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa8Y7hGhpeT5"
      },
      "source": [
        "We can download the data and create a PyTorch dataset using the `MNIST` class from `torchvision.datasets`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCfhGj-IpeT5"
      },
      "source": [
        "dataset = MNIST(root='data/', download=True, transform=ToTensor())"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeUQxXEjBw7k"
      },
      "source": [
        "Let's look at a couple of images from the dataset. The images are converted to PyTorch tensors with the shape `1x28x28` (the dimensions represent color channels, width and height). We can use `plt.imshow` to display the images. However, `plt.imshow` expects channels to be last dimension in an image tensor, so we use the `permute` method to reorder the dimensions of the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "id": "PuWnYYVkBw7k",
        "outputId": "211fedfc-a06c-45a0-fddc-bcbef61adc5d"
      },
      "source": [
        "image, label = dataset[0]\n",
        "print('image.shape:', image.shape)\n",
        "plt.imshow(image.permute(1, 2, 0), cmap='gray')\n",
        "print('Label:', label)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "image.shape: torch.Size([1, 28, 28])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-f75f24c010ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'image.shape:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Label:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2649\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         resample=resample, url=url, **({\"data\": data} if data is not\n\u001b[0;32m-> 2651\u001b[0;31m         None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2652\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5624\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5626\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5627\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    697\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[1;32m    698\u001b[0m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0;32m--> 699\u001b[0;31m                             .format(self._A.shape))\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Invalid shape (28, 28, 1) for image data"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz5CwsWxBw7l"
      },
      "source": [
        "image, label = dataset[0]\n",
        "print('image.shape:', image.shape)\n",
        "plt.imshow(image.permute(1, 2, 0), cmap='gray')\n",
        "print('Label:', label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vjwsyaDpeT5"
      },
      "source": [
        "Next, let's use the `random_split` helper function to set aside 10000 images for our validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cgek18qlpeT5"
      },
      "source": [
        "val_size = 10000\n",
        "train_size = len(dataset) - val_size\n",
        "\n",
        "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
        "len(train_ds), len(val_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxB8SiaYpeT6"
      },
      "source": [
        "We can now create PyTorch data loaders for training and validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cK_hoRYSpeT6"
      },
      "source": [
        "batch_size=128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iShTOn-vpeT6"
      },
      "source": [
        "train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjucG3lnpeT6"
      },
      "source": [
        "Can you figure out the purpose of the arguments `num_workers` and `pin_memory`? Try looking into the documentation: https://pytorch.org/docs/stable/data.html .\n",
        "\n",
        "Let's visualize a batch of data in a grid using the `make_grid` function from `torchvision`. We'll also use the `.permute` method on the tensor to move the channels to the last dimension, as expected by `matplotlib`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YshQj9srpeT6"
      },
      "source": [
        "for images, _ in train_loader:\n",
        "    print('images.shape:', images.shape)\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(make_grid(images, nrow=16).permute((1, 2, 0)))\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKEjn2zvpeT6"
      },
      "source": [
        "## Hidden Layers, Activation Functions and Non-Linearity\n",
        "\n",
        "We'll create a neural network with two layers: a _hidden layer_ and an _output layer_. Additionally, we'll use an _activation function_ between the two layers. Let's look at a step-by-step example to learn how hidden layers and activation functions can help capture non-linear relationships between inputs and outputs.\n",
        "\n",
        "First, let's create a batch of inputs tensors. We'll flatten the `1x28x28` images into vectors of size `784`, so they can be passed into an `nn.Linear` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5iPCdqEpeT6"
      },
      "source": [
        "for images, labels in train_loader:\n",
        "    print('images.shape:', images.shape)\n",
        "    inputs = images.reshape(-1, 784)\n",
        "    print('inputs.shape:', inputs.shape)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF__u2QfpeT6"
      },
      "source": [
        "Next, let's create a `nn.Linear` object, which will serve as our _hidden_ layer. We'll set the size of the output from the hidden layer to 32. This number can be increased or decreased to change the _learning capacity_ of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zXN1c4PpeT6"
      },
      "source": [
        "input_size = inputs.shape[-1]\n",
        "hidden_size = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOAh7CLZpeT7"
      },
      "source": [
        "layer1 = nn.Linear(input_size, hidden_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzqPBDuypeT7"
      },
      "source": [
        "We can now compute intermediate outputs for the batch of images by passing `inputs` through `layer1`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zd0YcDIlcCKb"
      },
      "source": [
        "inputs.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD_FPxeipeT7"
      },
      "source": [
        "layer1_outputs = layer1(inputs)\n",
        "print('layer1_outputs.shape:', layer1_outputs.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK-eA2QTpeT7"
      },
      "source": [
        "The image vectors of size `784` are transformed into intermediate output vectors of length `32` by performing a matrix multiplication of `inputs` matrix with the transposed weights matrix of `layer1` and adding the bias. We can verify this using `torch.allclose`. For a more detailed explanation, review the tutorial on [linear regression](https://jovian.ai/aakashns/02-linear-regression)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dEnp5VjpeT7"
      },
      "source": [
        "layer1_outputs_direct = inputs @ layer1.weight.t() + layer1.bias\n",
        "layer1_outputs_direct.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piqu77yOBw7s"
      },
      "source": [
        "torch.allclose(layer1_outputs, layer1_outputs_direct, 1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MW9IXDPtpeT7"
      },
      "source": [
        "Thus, `layer1_outputs` and `inputs` have a linear relationship, i.e., each element of `layer_outputs` is a weighted sum of elements from `inputs`. Thus, even as we train the model and modify the weights, `layer1` can only capture linear relationships between `inputs` and `outputs`.\n",
        "\n",
        "<img src=\"https://i.imgur.com/inXsLuq.png\" width=\"360\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vg4BbFtBw7t"
      },
      "source": [
        "Next, we'll use the Rectified Linear Unit (ReLU) function as the activation function for the outputs. It has the formula `relu(x) = max(0,x)` i.e. it simply replaces negative values in a given tensor with the value 0. ReLU is a non-linear function, as seen here visually:\n",
        "\n",
        "<img src=\"https://i.imgur.com/yijV4xF.png\" width=\"420\">\n",
        "\n",
        "We can use the `F.relu` method to apply ReLU to the elements of a tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHMpGUGRpeT7"
      },
      "source": [
        "F.relu(torch.tensor([[1, -1, 0], \n",
        "                     [-0.1, .2, 3]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0QIFoEopeT7"
      },
      "source": [
        "Let's apply the activation function to `layer1_outputs` and verify that negative values were replaced with 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqBj3KsnpeT7"
      },
      "source": [
        "relu_outputs = F.relu(layer1_outputs)\n",
        "print('min(layer1_outputs):', torch.min(layer1_outputs).item())\n",
        "print('min(relu_outputs):', torch.min(relu_outputs).item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g42i491NpeT7"
      },
      "source": [
        "Now that we've applied a non-linear activation function, `relu_outputs` and `inputs` do not have a linear relationship. We refer to `ReLU` as the _activation function_, because for each input certain outputs are activated (those with non-zero values) while others turned off (those with zero values)\n",
        "\n",
        "Next, let's create an output layer to convert vectors of length `hidden_size` in `relu_outputs` into vectors of length 10, which is the desired output of our model (since there are 10 target labels)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1qY-KYzpeT7"
      },
      "source": [
        "output_size = 10\n",
        "layer2 = nn.Linear(hidden_size, output_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_DuKvL8peT7"
      },
      "source": [
        "layer2_outputs = layer2(relu_outputs)\n",
        "print(layer2_outputs.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caxPuaS9dQhH"
      },
      "source": [
        "inputs.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AikFPlJppeT7"
      },
      "source": [
        "As expected, `layer2_outputs` contains a batch of vectors of size 10. We can now use this output to compute the loss using `F.cross_entropy` and adjust the weights of `layer1` and `layer2` using gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQ_ArLNApeT7"
      },
      "source": [
        "F.cross_entropy(layer2_outputs, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UazxyCKHpeT7"
      },
      "source": [
        "Thus, our model transforms `inputs` into `layer2_outputs` by applying a linear transformation (using `layer1`), followed by a non-linear activation (using `F.relu`), followed by another linear transformation (using `layer2`). Let's verify this by re-computing the output using basic matrix operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNQ2nPijpeT7"
      },
      "source": [
        "# Expanded version of layer2(F.relu(layer1(inputs)))\n",
        "outputs = (F.relu(inputs @ layer1.weight.t() + layer1.bias)) @ layer2.weight.t() + layer2.bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BgaRib-peT7"
      },
      "source": [
        "torch.allclose(outputs, layer2_outputs, 1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLOjL89WpeT7"
      },
      "source": [
        "Note that `outputs` and `inputs` do not have a linear relationship due to the non-linear activation function `F.relu`. As we train the model and adjust the weights of `layer1` and `layer2`, we can now capture non-linear relationships between the images and their labels. In other words, introducing non-linearity makes the model more powerful and versatile. Also, since `hidden_size` does not depend on the dimensions of the inputs or outputs, we vary it to increase the number of parameters within the model. We can also introduce new hidden layers and apply the same non-linear activation after each hidden layer.\n",
        "\n",
        "The model we just created is called a neural network. A _deep neural network_ is simply a neural network with one or more hidden layers. In fact, the [Universal Approximation Theorem](http://neuralnetworksanddeeplearning.com/chap4.html) states that a sufficiently large & deep neural network can compute any arbitrary function i.e. it can _learn_ rich and complex non-linear relationships between inputs and targets. Here are some examples:\n",
        "\n",
        "* Identifying if an image contains a cat or a dog (or [something else](https://machinelearningmastery.com/introduction-to-the-imagenet-large-scale-visual-recognition-challenge-ilsvrc/))\n",
        "* Identifying the genre of a song using a 10-second sample\n",
        "* Classifying movie reviews as positive or negative based on their content\n",
        "* Navigating self-driving cars using a video feed of the road\n",
        "* Translating sentences from English to French (and hundreds of other languages)\n",
        "* Converting a speech recording to text and vice versa\n",
        "* And many more...\n",
        "\n",
        "It's hard to imagine how the simple process of multiplying inputs with randomly initialized matrices, applying non-linear activations, and adjusting weights repeatedly using gradient descent can yield such astounding results. Deep learning models often contain millions of parameters, which can together capture far more complex relationships than the human brain can comprehend.\n",
        "\n",
        "If we hadn't included a non-linear activation between the two linear layers, the final relationship between inputs and outputs would still be linear. A simple refactoring of the computations illustrates this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHWruKwlpeT7"
      },
      "source": [
        "# Same as layer2(layer1(inputs))\n",
        "outputs2 = (inputs @ layer1.weight.t() + layer1.bias) @ layer2.weight.t() + layer2.bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTUU8qRRpeT8"
      },
      "source": [
        "# Create a single layer to replace the two linear layers\n",
        "combined_layer = nn.Linear(input_size, output_size)\n",
        "\n",
        "combined_layer.weight.data = layer2.weight @ layer1.weight\n",
        "combined_layer.bias.data = layer1.bias @ layer2.weight.t() + layer2.bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYR8OqE5Bw7z"
      },
      "source": [
        "# Same as combined_layer(inputs)\n",
        "outputs3 = inputs @ combined_layer.weight.t() + combined_layer.bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It7A4nzQpeT8"
      },
      "source": [
        "torch.allclose(outputs2, outputs3, 1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7NUCuW5wODZ"
      },
      "source": [
        "### Save and upload your notebook\n",
        "\n",
        "Whether you're running this Jupyter notebook online or on your computer, it's essential to save your work from time to time. You can continue working on a saved notebook later or share it with friends and colleagues to let them execute your code. [Jovian](https://jovian.ai/platform-features) offers an easy way of saving and sharing your Jupyter notebooks online."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4liggZ2uq-y"
      },
      "source": [
        "# Install the library\n",
        "!pip install jovian --upgrade --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsdoLFqSuquB"
      },
      "source": [
        "import jovian"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWyuhJvEwSJY"
      },
      "source": [
        "jovian.commit(project='04-feedforward-nn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur3bA-lZwZBu"
      },
      "source": [
        "`jovian.commit` uploads the notebook to your Jovian account, captures the Python environment, and creates a shareable link for your notebook, as shown above. You can use this link to share your work and let anyone (including you) run your notebooks and reproduce your work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8dOeJyfpeT8"
      },
      "source": [
        "## Model\n",
        "\n",
        "We are now ready to define our model. As discussed above, we'll create a neural network with one hidden layer. Here's what that means:\n",
        "\n",
        "* Instead of using a single `nn.Linear` object to transform a batch of inputs (pixel intensities) into outputs (class probabilities), we'll use two `nn.Linear` objects. Each of these is called a _layer_ in the network. \n",
        "\n",
        "* The first layer (also known as the hidden layer) will transform the input matrix of shape `batch_size x 784` into an intermediate output matrix of shape `batch_size x hidden_size`. The parameter `hidden_size` can be configured manually (e.g., 32 or 64).\n",
        "\n",
        "* We'll then apply a non-linear *activation function* to the intermediate outputs. The activation function transforms individual elements of the matrix.\n",
        "\n",
        "* The result of the activation function, which is also of size `batch_size x hidden_size`, is passed into the second layer (also known as the output layer).  The second layer transforms it into a matrix of size `batch_size x 10`. We can use this output to compute the loss and adjust weights using gradient descent.\n",
        "\n",
        "\n",
        "As discussed above, our model will contain one hidden layer. Here's what it looks like visually:\n",
        "\n",
        "<img src=\"https://i.imgur.com/eN7FrpF.png\" width=\"480\">\n",
        "\n",
        "\n",
        "Let's define the model by extending the `nn.Module` class from PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fKPW6tSpeT8"
      },
      "source": [
        "class MnistModel(nn.Module):\n",
        "    \"\"\"Feedfoward neural network with 1 hidden layer\"\"\"\n",
        "    def __init__(self, in_size, hidden_size, out_size):\n",
        "        super().__init__()\n",
        "        # hidden layer\n",
        "        self.linear1 = nn.Linear(in_size, hidden_size)\n",
        "        # output layer\n",
        "        self.linear2 = nn.Linear(hidden_size, out_size)\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        # Flatten the image tensors\n",
        "        xb = xb.view(xb.size(0), -1)\n",
        "        # Get intermediate outputs using hidden layer\n",
        "        out = self.linear1(xb)\n",
        "        # Apply activation function\n",
        "        out = F.relu(out)\n",
        "        # Get predictions using output layer\n",
        "        out = self.linear2(out)\n",
        "        return out\n",
        "    \n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                  # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                    # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
        "        acc = accuracy(out, labels)           # Calculate accuracy\n",
        "        return {'val_loss': loss, 'val_acc': acc}\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W8-63R6Bw72"
      },
      "source": [
        "We also need to define an `accuracy` function which calculates the accuracy of the model's prediction on an batch of inputs. It's used in `validation_step` above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooA0PwVipeT8"
      },
      "source": [
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sSd532VpeT8"
      },
      "source": [
        "We'll create a model that contains a hidden layer with 32 activations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beSRnIQTpeT8"
      },
      "source": [
        "input_size = 784\n",
        "hidden_size = 32 # you can change this\n",
        "num_classes = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvSp6BFppeT8"
      },
      "source": [
        "model = MnistModel(input_size, hidden_size=32, out_size=num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtWfv0N9peT8"
      },
      "source": [
        "Let's take a look at the model's parameters. We expect to see one weight and bias matrix for each of the layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psecOJfZpeT8"
      },
      "source": [
        "for t in model.parameters():\n",
        "    print(t.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeFGsljIpeT8"
      },
      "source": [
        "Let's try and generate some outputs using our model. We'll take the first batch of 128 images from our dataset and pass them into our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jajA6VFYpeT8"
      },
      "source": [
        "for images, labels in train_loader:\n",
        "    outputs = model(images)\n",
        "    loss = F.cross_entropy(outputs, labels)\n",
        "    print('Loss:', loss.item())\n",
        "    break\n",
        "\n",
        "print('outputs.shape : ', outputs.shape)\n",
        "print('Sample outputs :\\n', outputs[:2].data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmw6iiTXpeT8"
      },
      "source": [
        "## Using a GPU\n",
        "\n",
        "As the sizes of our models and datasets increase, we need to use GPUs to train our models within a reasonable amount of time. GPUs contain hundreds of cores optimized for performing expensive matrix operations on floating-point numbers quickly, making them ideal for training deep neural networks. You can use GPUs for free on [Google Colab](https://colab.research.google.com/) and [Kaggle](https://www.kaggle.com/kernels) or rent GPU-powered machines on services like [Google Cloud Platform](https://cloud.google.com/gpu/), [Amazon Web Services](https://docs.aws.amazon.com/dlami/latest/devguide/gpu.html), and [Paperspace](https://www.paperspace.com/).\n",
        "\n",
        "We can check if a GPU is available and the required NVIDIA CUDA drivers are installed using `torch.cuda.is_available`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt2Ve_cFpeT8"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2ffHXrnpeT8"
      },
      "source": [
        "Let's define a helper function to ensure that our code uses the GPU if available and defaults to using the CPU if it isn't. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oY8kxA4tpeT8"
      },
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgxTDAjtpeT8",
        "outputId": "1a7e1de0-138c-49a6-dbcd-46ee6416dbb6"
      },
      "source": [
        "device = get_default_device()\n",
        "device"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U58VabR4peT8"
      },
      "source": [
        "Next, let's define a function that can move data and model to a chosen device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qe39OMPmpeT8"
      },
      "source": [
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdouJFtwpeT8",
        "outputId": "c2b50821-f2bf-47d4-9387-1aee44cf7615"
      },
      "source": [
        "for images, labels in train_loader:\n",
        "    print(images.shape)\n",
        "    images = to_device(images, device)\n",
        "    print(images.device)\n",
        "    break"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 1, 28, 28])\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiKbIzujpeT9"
      },
      "source": [
        "Finally, we define a `DeviceDataLoader` class to wrap our existing data loaders and move batches of data to the selected device. Interestingly, we don't need to extend an existing class to create a PyTorch datal oader. All we need is an `__iter__` method to retrieve batches of data and an `__len__` method to get the number of batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mncx5AEcpeT9"
      },
      "source": [
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxwvU5eP4r76"
      },
      "source": [
        "The `yield` keyword in Python is used to create a generator function that can be used within a `for` loop, as illustrated below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IeHPpx74slh",
        "outputId": "4bf11c6c-4960-4ff2-ebfa-cf13d2584601"
      },
      "source": [
        "def some_numbers():\n",
        "    yield 10\n",
        "    yield 20\n",
        "    yield 30\n",
        "\n",
        "for value in some_numbers():\n",
        "    print(value)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "20\n",
            "30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AM28h0ipeT9"
      },
      "source": [
        "We can now wrap our data loaders using `DeviceDataLoader`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwZdTIxgpeT9"
      },
      "source": [
        "train_loader = DeviceDataLoader(train_loader, device)\n",
        "val_loader = DeviceDataLoader(val_loader, device)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go0zbQTypeT9"
      },
      "source": [
        "Tensors moved to the GPU have a `device` property which includes that word `cuda`. Let's verify this by looking at a batch of data from `valid_dl`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdOfMdmxpeT9",
        "outputId": "37f840fa-b640-4950-c40a-9e08fa674d14"
      },
      "source": [
        "for xb, yb in val_loader:\n",
        "    print('xb.device:', xb.device)\n",
        "    print('yb:', yb)\n",
        "    break"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "xb.device: cuda:0\n",
            "yb: tensor([8, 0, 8, 7, 0, 8, 6, 8, 2, 3, 7, 5, 4, 9, 4, 4, 3, 5, 9, 1, 3, 9, 3, 4,\n",
            "        8, 3, 3, 3, 7, 8, 2, 3, 0, 3, 5, 2, 9, 3, 9, 5, 4, 1, 2, 2, 4, 2, 2, 7,\n",
            "        8, 1, 0, 5, 5, 9, 6, 1, 6, 3, 9, 8, 3, 9, 3, 9, 2, 5, 3, 8, 2, 9, 2, 5,\n",
            "        6, 8, 5, 7, 4, 8, 7, 9, 3, 7, 7, 9, 1, 2, 7, 7, 8, 2, 3, 3, 4, 6, 8, 8,\n",
            "        8, 9, 2, 6, 5, 1, 6, 5, 1, 7, 0, 2, 3, 7, 7, 1, 3, 7, 6, 4, 9, 9, 2, 7,\n",
            "        6, 9, 6, 7, 7, 2, 6, 9, 0, 6, 6, 5, 2, 4, 3, 9, 4, 4, 1, 3, 8, 0, 0, 3,\n",
            "        6, 0, 4, 2, 1, 9, 2, 6, 7, 7, 6, 8, 2, 6, 4, 3, 6, 9, 4, 1, 4, 2, 9, 2,\n",
            "        6, 0, 6, 9, 2, 6, 3, 8, 7, 1, 4, 9, 0, 9, 6, 3, 9, 0, 4, 7, 4, 7, 7, 6,\n",
            "        1, 4, 0, 1, 9, 5, 6, 1, 4, 3, 1, 3, 0, 1, 0, 5, 4, 2, 1, 1, 1, 8, 4, 3,\n",
            "        2, 5, 0, 0, 2, 7, 0, 9, 7, 9, 7, 5, 6, 7, 6, 3, 7, 7, 3, 2, 3, 8, 5, 3,\n",
            "        1, 8, 9, 9, 1, 2, 2, 0, 0, 4, 3, 4, 6, 3, 2, 1], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfPhGrIRpeT9"
      },
      "source": [
        "## Training the Model\n",
        "\n",
        "We'll define two functions: `fit` and `evaluate` to train the model using gradient descent and evaluate its performance on the validation set. For a detailed walkthrough of these functions, check out the [previous tutorial](https://jovian.ai/aakashns/03-logistic-regression)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRw6Zf0wpeT9"
      },
      "source": [
        "def evaluate(model, val_loader):\n",
        "    \"\"\"Evaluate the model's performance on the validation set\"\"\"\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
        "    \"\"\"Train the model using gradient descent\"\"\"\n",
        "    history = []\n",
        "    optimizer = opt_func(model.parameters(), lr)\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase \n",
        "        for batch in train_loader:\n",
        "            loss = model.training_step(batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    return history"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuWv6GKZpeT9"
      },
      "source": [
        "Before we train the model, we need to ensure that the data and the model's parameters (weights and biases) are on the same device (CPU or GPU). We can reuse the `to_device` function to move the model's parameters to the right device. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1CyEQsJpeT9",
        "outputId": "3ecc3b9b-59c1-4fec-a7fb-1e1187c983a7"
      },
      "source": [
        "# Model (on GPU)\n",
        "model = MnistModel(input_size, hidden_size=hidden_size, out_size=num_classes)\n",
        "to_device(model, device)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MnistModel(\n",
              "  (linear1): Linear(in_features=784, out_features=32, bias=True)\n",
              "  (linear2): Linear(in_features=32, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N_aE2GRpeT9"
      },
      "source": [
        "Let's see how the model performs on the validation set with the initial set of weights and biases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bF_wEBAIpeT9",
        "outputId": "20ed2338-175f-494e-e522-a9212dfa736b"
      },
      "source": [
        "history = [evaluate(model, val_loader)]\n",
        "history"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'val_acc': 0.08037109673023224, 'val_loss': 2.317824125289917}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU0tYFcPpeT9"
      },
      "source": [
        "The initial accuracy is around 10%, as one might expect from a randomly initialized model (since it has a 1 in 10 chance of getting a label right by guessing randomly).\n",
        "\n",
        "Let's train the model for five epochs and look at the results. We can use a relatively high learning rate of 0.5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE5_oVckpeT9",
        "outputId": "9bd23f8d-9600-45c5-9372-31402614c3ed"
      },
      "source": [
        "history += fit(5, 0.5, model, train_loader, val_loader)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [0], val_loss: 0.2367, val_acc: 0.9286\n",
            "Epoch [1], val_loss: 0.1851, val_acc: 0.9432\n",
            "Epoch [2], val_loss: 0.1616, val_acc: 0.9501\n",
            "Epoch [3], val_loss: 0.1707, val_acc: 0.9457\n",
            "Epoch [4], val_loss: 0.1485, val_acc: 0.9525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_by-a2g9peT9"
      },
      "source": [
        "96% is pretty good! Let's train the model for five more epochs at a lower learning rate of 0.1 to improve the accuracy further."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjA2dtlrpeT9",
        "outputId": "8b73dc8c-e336-4b1b-b163-a66a3163c291"
      },
      "source": [
        "history += fit(5, 0.1, model, train_loader, val_loader)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [0], val_loss: 0.1311, val_acc: 0.9585\n",
            "Epoch [1], val_loss: 0.1304, val_acc: 0.9593\n",
            "Epoch [2], val_loss: 0.1308, val_acc: 0.9586\n",
            "Epoch [3], val_loss: 0.1313, val_acc: 0.9570\n",
            "Epoch [4], val_loss: 0.1310, val_acc: 0.9603\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7LAqJCMpeT9"
      },
      "source": [
        "We can now plot the losses & accuracies to study how the model improves over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "I3g-DRifpeT9",
        "outputId": "d9b12068-3c78-4164-d772-127a0777dabd"
      },
      "source": [
        "losses = [x['val_loss'] for x in history]\n",
        "plt.plot(losses, '-x')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.title('Loss vs. No. of epochs');"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "28kJvNmFpeT9",
        "scrolled": true,
        "outputId": "e7dfd9c7-82a5-4b70-a905-0370ce6815a2"
      },
      "source": [
        "accuracies = [x['val_acc'] for x in history]\n",
        "plt.plot(accuracies, '-x')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('Accuracy vs. No. of epochs');"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBmLGRuLpeT9"
      },
      "source": [
        "Our current model outperforms the logistic regression model (which could only achieve around 86% accuracy) by a considerable margin! It quickly reaches an accuracy of 97% but doesn't improve much beyond this. To improve accuracy further, we need to make the model more powerful by increasing the hidden layer's size or adding more hidden layers with activations. I encourage you to try out both these approaches and see which one works better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v72CM4EYpeT9"
      },
      "source": [
        "As a final step, we can save and commit our work using the `jovian` library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oF5BstrpeT9"
      },
      "source": [
        "!pip install jovian --upgrade -q"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqmI7j7TpeT-"
      },
      "source": [
        "import jovian"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koPOVc5ipeT-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "13b3ddfe-dd21-4341-d8cd-2b0f9e20563a"
      },
      "source": [
        "jovian.commit(project='04-feedforward-nn', environment=None)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[jovian] Detected Colab notebook...\u001b[0m\n",
            "[jovian] Uploading colab notebook to Jovian...\u001b[0m\n",
            "[jovian] Committed successfully! https://jovian.ai/rezvanizahra/04-feedforward-nn\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://jovian.ai/rezvanizahra/04-feedforward-nn'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lf_6nWkCBGB5"
      },
      "source": [
        "## Testing with individual images\n",
        "\n",
        "While we have been tracking the overall accuracy of a model so far, it's also a good idea to look at model's results on some sample images. Let's test out our model with some images from the predefined test dataset of 10000 images. We begin by recreating the test dataset with the `ToTensor` transform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJ6E4f-iBBTm"
      },
      "source": [
        "# Define test dataset\n",
        "test_dataset = MNIST(root='data/', \n",
        "                     train=False,\n",
        "                     transform=ToTensor())"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGKvjeJXBTiN"
      },
      "source": [
        "Let's define a helper function `predict_image`, which returns the predicted label for a single image tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhxrAx6LBTNG"
      },
      "source": [
        "def predict_image(img, model):\n",
        "    xb = to_device(img.unsqueeze(0), device)\n",
        "    yb = model(xb)\n",
        "    _, preds  = torch.max(yb, dim=1)\n",
        "    return preds[0].item()"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4kUaLBqBZLd"
      },
      "source": [
        "Let's try it out with a few images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsNnl42DBXog",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "4e89500d-422f-4eaa-cafd-4da7ad9369ed"
      },
      "source": [
        "img, label = test_dataset[0]\n",
        "plt.imshow(img[0], cmap='gray')\n",
        "print('Label:', label, ', Predicted:', predict_image(img, model))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 7 , Predicted: 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lu0lN0xnBiD6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "48f382ed-2d3e-4acc-ac1d-be9ee93cf1ce"
      },
      "source": [
        "img, label = test_dataset[1839]\n",
        "plt.imshow(img[0], cmap='gray')\n",
        "print('Label:', label, ', Predicted:', predict_image(img, model))"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 2 , Predicted: 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpzxiYHjBliK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "de786594-7e3d-4c17-8e24-45dc19a29b81"
      },
      "source": [
        "img, label = test_dataset[193]\n",
        "plt.imshow(img[0], cmap='gray')\n",
        "print('Label:', label, ', Predicted:', predict_image(img, model))"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 9 , Predicted: 9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqG62k05BqpC"
      },
      "source": [
        "Identifying where our model performs poorly can help us improve the model, by collecting more training data, increasing/decreasing the complexity of the model, and changing the hypeparameters.\n",
        "\n",
        "As a final step, let's also look at the overall loss and accuracy of the model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvNoyn0RBtBm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "281ed99c-15bf-490a-fa59-c248b68e2efb"
      },
      "source": [
        "test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size=256), device)\n",
        "result = evaluate(model, test_loader)\n",
        "result"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'val_acc': 0.9668945074081421, 'val_loss': 0.11501242220401764}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZz2M4WPBzlT"
      },
      "source": [
        "We expect this to be similar to the accuracy/loss on the validation set. If not, we might need a better validation set that has similar data and distribution as the test set (which often comes from real world data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-syIZYUB1Ls"
      },
      "source": [
        "Let's save the model's weights and attach it to the notebook using `jovian.commit`. We will also record the model's performance on the test dataset using `jovian.log_metrics`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DhlvKaeCE0K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b78ec8a6-ceb0-4f6d-f1b7-d3c6c07a0bcd"
      },
      "source": [
        "jovian.log_metrics(test_loss=result['val_loss'], test_acc=result['val_loss'])"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[jovian] Metrics logged.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT6v860WByIJ"
      },
      "source": [
        "torch.save(model.state_dict(), 'mnist-feedforward.pth')"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzsA_y7OCLY8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "c09c77b5-669d-4708-8fc5-61e2071c1595"
      },
      "source": [
        "jovian.commit(project='04-feedforward-nn', \n",
        "              environment=None, \n",
        "              outputs=['mnist-feedforward.pth'])"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[jovian] Detected Colab notebook...\u001b[0m\n",
            "[jovian] Uploading colab notebook to Jovian...\u001b[0m\n",
            "[jovian] Uploading additional outputs...\u001b[0m\n",
            "[jovian] Attaching records (metrics, hyperparameters, dataset etc.)\u001b[0m\n",
            "[jovian] Committed successfully! https://jovian.ai/rezvanizahra/04-feedforward-nn\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://jovian.ai/rezvanizahra/04-feedforward-nn'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPk8QvFo9z3C"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "Try out the following exercises to apply the concepts and techniques you have learned so far:\n",
        "\n",
        "* Coding exercises on end-to-end model training: https://jovian.ai/aakashns/03-cifar10-feedforward\n",
        "* Starter notebook for deep learning models:  https://jovian.ai/aakashns/fashion-feedforward-minimal\n",
        "\n",
        "Training great machine learning models reliably takes practice and experience. Try experimenting with different datasets, models and hyperparameters, it's the best way to acquire this skill."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSSOi-g_peT-"
      },
      "source": [
        "## Summary and Further Reading\n",
        "\n",
        "Here is a summary of the topics covered in this tutorial:\n",
        "\n",
        "* We created a neural network with one hidden layer to improve upon the logistic regression model from the previous tutorial. We also used the ReLU activation function to introduce non-linearity into the model, allowing it to learn more complex relationships between the inputs (pixel densities) and outputs (class probabilities).\n",
        "\n",
        "* We defined some utilities like `get_default_device`, `to_device` and `DeviceDataLoader` to leverage a GPU if available, by moving the input data and model parameters to the appropriate device.\n",
        "\n",
        "* We were able to use the exact same training loop: the `fit` function we had define earlier to train out model and evaluate it using the validation dataset.\n",
        "\n",
        "There's a lot of scope to experiment here, and I encourage you to use the interactive nature of Jupyter to play around with the various parameters. Here are a few ideas:\n",
        "\n",
        "* Try changing the size of the hidden layer, or add more hidden layers and see if you can achieve a higher accuracy.\n",
        "\n",
        "* Try changing the batch size and learning rate to see if you can achieve the same accuracy in fewer epochs.\n",
        "\n",
        "* Compare the training times on a CPU vs. GPU. Do you see a significant difference. How does it vary with the size of the dataset and the size of the model (no. of weights and parameters)?\n",
        "\n",
        "* Try building a model for a different dataset, such as the [CIFAR10 or CIFAR100 datasets](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
        "\n",
        "Here are some references for further reading:\n",
        "\n",
        "* [A visual proof that neural networks can compute any function](http://neuralnetworksanddeeplearning.com/chap4.html), also known as the Universal Approximation Theorem.\n",
        "\n",
        "* [But what *is* a neural network?](https://www.youtube.com/watch?v=aircAruvnKk) - A visual and intuitive introduction to what neural networks are and what the intermediate layers represent\n",
        "\n",
        "* [Stanford CS229 Lecture notes on Backpropagation](http://cs229.stanford.edu/notes/cs229-notes-backprop.pdf) - for a more mathematical treatment of how gradients are calculated and weights are updated for neural networks with multiple layers.\n",
        "\n",
        "\n",
        "You are now ready to move on to the next tutorial: [Image Classification using Convolutional Neural Networks](https://jovian.ai/aakashns/05-cifar10-cnn)."
      ]
    }
  ]
}